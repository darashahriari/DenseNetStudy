{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DenseNet-Modified-reduced-samples-optimizer-adam.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-nxNPzE3yDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from models import DenseNet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1qP7U3n49Xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class AverageMeter(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def train_epoch(model, loader, optimizer, epoch, n_epochs, print_freq=10):\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    error = AverageMeter()\n",
        "\n",
        "    # Model on train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for batch_idx, (input, target) in enumerate(loader):\n",
        "        # Create vaiables\n",
        "        if torch.cuda.is_available():\n",
        "            input = input.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "        # compute output\n",
        "        output = model(input)\n",
        "        loss = torch.nn.functional.cross_entropy(output, target)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        batch_size = target.size(0)\n",
        "        _, pred = output.data.cpu().topk(1, dim=1)\n",
        "        error.update(torch.ne(pred.squeeze(), target.cpu()).float().sum().item() / batch_size, batch_size)\n",
        "        losses.update(loss.item(), batch_size)\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        # print stats\n",
        "        if batch_idx % print_freq == 0:\n",
        "            res = '\\t'.join([\n",
        "                'Epoch: [%d/%d]' % (epoch + 1, n_epochs),\n",
        "                'Iter: [%d/%d]' % (batch_idx + 1, len(loader)),\n",
        "                'Time %.3f (%.3f)' % (batch_time.val, batch_time.avg),\n",
        "                'Loss %.4f (%.4f)' % (losses.val, losses.avg),\n",
        "                'Error %.4f (%.4f)' % (error.val, error.avg),\n",
        "            ])\n",
        "            print(res)\n",
        "\n",
        "    # Return summary statistics\n",
        "    return batch_time.avg, losses.avg, error.avg\n",
        "\n",
        "\n",
        "def test_epoch(model, loader, print_freq=10, is_test=True):\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    error = AverageMeter()\n",
        "\n",
        "    # Model on eval mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (input, target) in enumerate(loader):\n",
        "            # Create vaiables\n",
        "            if torch.cuda.is_available():\n",
        "                input = input.cuda()\n",
        "                target = target.cuda()\n",
        "\n",
        "            # compute output\n",
        "            output = model(input)\n",
        "            loss = torch.nn.functional.cross_entropy(output, target)\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            batch_size = target.size(0)\n",
        "            _, pred = output.data.cpu().topk(1, dim=1)\n",
        "            error.update(torch.ne(pred.squeeze(), target.cpu()).float().sum().item() / batch_size, batch_size)\n",
        "            losses.update(loss.item(), batch_size)\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            # print stats\n",
        "            if batch_idx % print_freq == 0:\n",
        "                res = '\\t'.join([\n",
        "                    'Test' if is_test else 'Valid',\n",
        "                    'Iter: [%d/%d]' % (batch_idx + 1, len(loader)),\n",
        "                    'Time %.3f (%.3f)' % (batch_time.val, batch_time.avg),\n",
        "                    'Loss %.4f (%.4f)' % (losses.val, losses.avg),\n",
        "                    'Error %.4f (%.4f)' % (error.val, error.avg),\n",
        "                ])\n",
        "                print(res)\n",
        "\n",
        "    # Return summary statistics\n",
        "    return batch_time.avg, losses.avg, error.avg\n",
        "\n",
        "\n",
        "def train(model, train_set, valid_set, test_set, save, n_epochs=100,\n",
        "          batch_size=64, lr=0.1, wd=0.0001, momentum=0.9, seed=None):\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True,\n",
        "                                               pin_memory=(torch.cuda.is_available()), num_workers=0)\n",
        "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False,\n",
        "                                              pin_memory=(torch.cuda.is_available()), num_workers=0)\n",
        "    if valid_set is None:\n",
        "        valid_loader = None\n",
        "    else:\n",
        "        valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False,\n",
        "                                                   pin_memory=(torch.cuda.is_available()), num_workers=0)\n",
        "    # Model on cuda\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "\n",
        "    # Wrap model for multi-GPUs, if necessary\n",
        "    model_wrapper = model\n",
        "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
        "        model_wrapper = torch.nn.DataParallel(model).cuda()\n",
        "\n",
        "    # Optimizer\n",
        "    #optimizer = torch.optim.SGD(model_wrapper.parameters(), lr=lr, momentum=momentum, nesterov=True, weight_decay=wd)\n",
        "    optimizer = torch.optim.Adam(model_wrapper.parameters(), lr=lr, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[0.5 * n_epochs, 0.75 * n_epochs],\n",
        "                                                     gamma=0.1)\n",
        "\n",
        "    # Start log\n",
        "    with open(os.path.join(save, 'results.csv'), 'w') as f:\n",
        "        f.write('epoch,train_loss,train_error,valid_loss,valid_error,test_error\\n')\n",
        "\n",
        "    # Train model\n",
        "    best_error = 1\n",
        "    for epoch in range(n_epochs):\n",
        "        scheduler.step()\n",
        "        _, train_loss, train_error = train_epoch(\n",
        "            model=model_wrapper,\n",
        "            loader=train_loader,\n",
        "            optimizer=optimizer,\n",
        "            epoch=epoch,\n",
        "            n_epochs=n_epochs,\n",
        "        )\n",
        "        _, valid_loss, valid_error = test_epoch(\n",
        "            model=model_wrapper,\n",
        "            loader=valid_loader if valid_loader else test_loader,\n",
        "            is_test=(not valid_loader)\n",
        "        )\n",
        "\n",
        "        # Determine if model is the best\n",
        "        if valid_loader:\n",
        "            if valid_error < best_error:\n",
        "                best_error = valid_error\n",
        "                print('New best error: %.4f' % best_error)\n",
        "                torch.save(model.state_dict(), os.path.join(save, 'model.dat'))\n",
        "        else:\n",
        "            torch.save(model.state_dict(), os.path.join(save, 'model.dat'))\n",
        "\n",
        "        # Log results\n",
        "        with open(os.path.join(save, 'results.csv'), 'a') as f:\n",
        "            f.write('%03d,%0.6f,%0.6f,%0.5f,%0.5f,\\n' % (\n",
        "                (epoch + 1),\n",
        "                train_loss,\n",
        "                train_error,\n",
        "                valid_loss,\n",
        "                valid_error,\n",
        "            ))\n",
        "\n",
        "    # Final test of model on test set\n",
        "    model.load_state_dict(torch.load(os.path.join(save, 'model.dat')))\n",
        "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
        "        model = torch.nn.DataParallel(model).cuda()\n",
        "    test_results = test_epoch(\n",
        "        model=model,\n",
        "        loader=test_loader,\n",
        "        is_test=True\n",
        "    )\n",
        "    _, _, test_error = test_results\n",
        "    with open(os.path.join(save, 'results.csv'), 'a') as f:\n",
        "        f.write(',,,,,%0.5f\\n' % (test_error))\n",
        "    print('Final test error: %.4f' % test_error)\n",
        "\n",
        "\n",
        "def demo(data, save, depth=100, growth_rate=12, efficient=True, valid_size=5000,\n",
        "         n_epochs=100, batch_size=64, seed=None):\n",
        "\n",
        "\n",
        "    # Get densenet configuration\n",
        "    if (depth - 4) % 3:\n",
        "        raise Exception('Invalid depth')\n",
        "    block_config = [(depth - 4) // 6 for _ in range(3)]\n",
        "\n",
        "    # Data transforms\n",
        "    mean = [0.5071, 0.4867, 0.4408]\n",
        "    stdv = [0.2675, 0.2565, 0.2761]\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=stdv),\n",
        "    ])\n",
        "    test_transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=stdv),\n",
        "    ])\n",
        "\n",
        "    # Datasets\n",
        "    train_set = datasets.CIFAR10(data, train=True, transform=train_transforms, download=True)\n",
        "    test_set = datasets.CIFAR10(data, train=False, transform=test_transforms, download=False)\n",
        "\n",
        "    if valid_size:\n",
        "        valid_set = datasets.CIFAR10(data, train=True, transform=test_transforms)\n",
        "        indices = torch.randperm(len(train_set))\n",
        "        #train_indices = indices[:len(indices) - valid_size]\n",
        "        train_indices = indices[:round(len(indices)*0.5)]\n",
        "        #valid_indices = indices[len(indices) - valid_size:]\n",
        "        valid_indices = indices[round(len(indices)*0.5) + 1:(round(len(indices)*0.5) + 1) + valid_size]\n",
        "        print(valid_indices.size())\n",
        "        train_set = torch.utils.data.Subset(train_set, train_indices)\n",
        "        valid_set = torch.utils.data.Subset(valid_set, valid_indices)\n",
        "    else:\n",
        "        valid_set = None\n",
        "\n",
        "    # Models\n",
        "    model = DenseNet(\n",
        "        growth_rate=growth_rate,\n",
        "        block_config=block_config,\n",
        "        num_classes=10,\n",
        "        small_inputs=True,\n",
        "        efficient=efficient,\n",
        "    )\n",
        "    print(model)\n",
        "\n",
        "    # Make save directory\n",
        "    if not os.path.exists(save):\n",
        "        os.makedirs(save)\n",
        "    if not os.path.isdir(save):\n",
        "        raise Exception('%s is not a dir' % save)\n",
        "\n",
        "    # Train the model\n",
        "    train(model=model, train_set=train_set, valid_set=valid_set, test_set=test_set, save=save,\n",
        "          n_epochs=n_epochs, batch_size=batch_size, seed=seed)\n",
        "    print('Done!')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "demo('cifar10', './', depth=100, growth_rate=12, efficient=True, valid_size=5000,\n",
        "         n_epochs=100, batch_size=64, seed=None)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}