{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet50_Project4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2UgYgkRuLyB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "fdf69565-3c47-4dc6-9773-14e2133bb212"
      },
      "source": [
        "from torchvision.models.resnet import ResNet, BasicBlock\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import inspect\n",
        "import time\n",
        "from torch import nn, optim\n",
        "import torch\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
        "from tqdm.autonotebook import tqdm\n",
        "import numpy as np\t\t\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.utils.data as data_utils\n",
        "from torch.autograd import Variable\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from functools import partial\n",
        "from dataclasses import dataclass\n",
        "from collections import OrderedDict\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "import time\n",
        "import torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGk7RzKwu1es",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = 'cifar10'\n",
        "batch_size = 64\n",
        "valid_size = 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgvEVuSfvnWz",
        "colab_type": "code",
        "outputId": "0f3e6df0-a89f-413c-a3f9-953352ab591f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "mean = [0.5071, 0.4867, 0.4408]\n",
        "stdv = [0.2675, 0.2565, 0.2761]\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=stdv),\n",
        "])\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=stdv),\n",
        "])\n",
        "\n",
        "train_set = datasets.CIFAR10(data, train=True, transform=train_transforms, download=True)\n",
        "test_set = datasets.CIFAR10(data, train=False, transform=test_transforms, download=False)\n",
        "\n",
        "if valid_size:\n",
        "        valid_set = datasets.CIFAR10(data, train=True, transform=test_transforms)\n",
        "        indices = torch.randperm(len(train_set))\n",
        "        #train_indices = indices[:len(indices) - valid_size]\n",
        "        train_indices = indices[:round(len(indices)*0.5)]\n",
        "        #valid_indices = indices[len(indices) - valid_size:]\n",
        "        valid_indices = indices[round(len(indices)*0.5) + 1:(round(len(indices)*0.5) + 1) + valid_size]\n",
        "        train_set = torch.utils.data.Subset(train_set, train_indices)\n",
        "        valid_set = torch.utils.data.Subset(valid_set, valid_indices)\n",
        "else:\n",
        "        valid_set = None"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to cifar10/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 99%|█████████▉| 169402368/170498071 [00:11<00:00, 33304245.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting cifar10/cifar-10-python.tar.gz to cifar10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98YUWvBGuQi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=(torch.cuda.is_available()), num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, pin_memory=(torch.cuda.is_available()), num_workers=0)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False, pin_memory=(torch.cuda.is_available()), num_workers=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1AEDaKTuqXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Conv2dAuto(nn.Conv2d):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.padding =  (self.kernel_size[0] // 2, self.kernel_size[1] // 2) # dynamic add padding based on the kernel_size\n",
        "        \n",
        "conv3x3 = partial(Conv2dAuto, kernel_size=3, bias=False)\n",
        "\n",
        "def activation_func(activation):\n",
        "    return  nn.ModuleDict([\n",
        "        ['relu', nn.ReLU(inplace=True)],\n",
        "        ['leaky_relu', nn.LeakyReLU(negative_slope=0.01, inplace=True)],\n",
        "        ['selu', nn.SELU(inplace=True)],\n",
        "        ['none', nn.Identity()]\n",
        "    ])[activation]\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, activation='relu'):\n",
        "        super().__init__()\n",
        "        self.in_channels, self.out_channels, self.activation = in_channels, out_channels, activation\n",
        "        self.blocks = nn.Identity()\n",
        "        self.activate = activation_func(activation)\n",
        "        self.shortcut = nn.Identity()   \n",
        "    \n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        if self.should_apply_shortcut: residual = self.shortcut(x)\n",
        "        x = self.blocks(x)\n",
        "        x += residual\n",
        "        x = self.activate(x)\n",
        "        return x\n",
        "    \n",
        "    @property\n",
        "    def should_apply_shortcut(self):\n",
        "        return self.in_channels != self.out_channels\n",
        "\n",
        "class ResNetResidualBlock(ResidualBlock):\n",
        "    def __init__(self, in_channels, out_channels, expansion=1, downsampling=1, conv=conv3x3, *args, **kwargs):\n",
        "        super().__init__(in_channels, out_channels)\n",
        "        self.expansion, self.downsampling, self.conv = expansion, downsampling, conv\n",
        "        self.shortcut = nn.Sequential(\n",
        "            nn.Conv2d(self.in_channels, self.expanded_channels, kernel_size=1,\n",
        "                      stride=self.downsampling, bias=False),\n",
        "            nn.BatchNorm2d(self.expanded_channels)) if self.should_apply_shortcut else None\n",
        "        \n",
        "        \n",
        "    @property\n",
        "    def expanded_channels(self):\n",
        "        return self.out_channels * self.expansion\n",
        "    \n",
        "    @property\n",
        "    def should_apply_shortcut(self):\n",
        "        return self.in_channels != self.expanded_channels\n",
        "\n",
        "\n",
        "def conv_bn(in_channels, out_channels, conv, *args, **kwargs):\n",
        "    return nn.Sequential(conv(in_channels, out_channels, *args, **kwargs), nn.BatchNorm2d(out_channels))\n",
        "\n",
        "class ResNetBasicBlock(ResNetResidualBlock):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
        "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
        "        self.blocks = nn.Sequential(\n",
        "            conv_bn(self.in_channels, self.out_channels, conv=self.conv, bias=False, stride=self.downsampling),\n",
        "            activation_func(self.activation),\n",
        "            conv_bn(self.out_channels, self.expanded_channels, conv=self.conv, bias=False),\n",
        "        )\n",
        "\n",
        "class ResNetBottleNeckBlock(ResNetResidualBlock):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
        "        super().__init__(in_channels, out_channels, expansion=4, *args, **kwargs)\n",
        "        self.blocks = nn.Sequential(\n",
        "           conv_bn(self.in_channels, self.out_channels, self.conv, kernel_size=1),\n",
        "             activation_func(self.activation),\n",
        "             conv_bn(self.out_channels, self.out_channels, self.conv, kernel_size=3, stride=self.downsampling),\n",
        "             activation_func(self.activation),\n",
        "             conv_bn(self.out_channels, self.expanded_channels, self.conv, kernel_size=1),\n",
        "        )\n",
        "\n",
        "class ResNetLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, block=ResNetBasicBlock, n=1, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        # 'We perform downsampling directly by convolutional layers that have a stride of 2.'\n",
        "        downsampling = 2 if in_channels != out_channels else 1\n",
        "        \n",
        "        self.blocks = nn.Sequential(\n",
        "            block(in_channels , out_channels, *args, **kwargs, downsampling=downsampling),\n",
        "            *[block(out_channels * block.expansion, \n",
        "                    out_channels, downsampling=1, *args, **kwargs) for _ in range(n - 1)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.blocks(x)\n",
        "        return x\n",
        "\n",
        "class ResNetEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=3, blocks_sizes=[64, 128, 256, 512], deepths=[2,2,2,2], \n",
        "                 activation='relu', block=ResNetBasicBlock, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.blocks_sizes = blocks_sizes\n",
        "        \n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.blocks_sizes[0], kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(self.blocks_sizes[0]),\n",
        "            activation_func(activation),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "        \n",
        "        self.in_out_block_sizes = list(zip(blocks_sizes, blocks_sizes[1:]))\n",
        "        self.blocks = nn.ModuleList([ \n",
        "            ResNetLayer(blocks_sizes[0], blocks_sizes[0], n=deepths[0], activation=activation, \n",
        "                        block=block,*args, **kwargs),\n",
        "            *[ResNetLayer(in_channels * block.expansion, \n",
        "                          out_channels, n=n, activation=activation, \n",
        "                          block=block, *args, **kwargs) \n",
        "              for (in_channels, out_channels), n in zip(self.in_out_block_sizes, deepths[1:])]       \n",
        "        ])\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.gate(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResnetDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features, n_classes):\n",
        "        super().__init__()\n",
        "        self.avg = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.decoder = nn.Linear(in_features, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avg(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels, n_classes, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.encoder = ResNetEncoder(in_channels, *args, **kwargs)\n",
        "        self.decoder = ResnetDecoder(self.encoder.blocks[-1].blocks[-1].expanded_channels, n_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def resnet18(in_channels, n_classes):\n",
        "    return ResNet(in_channels, n_classes, block=ResNetBasicBlock, deepths=[2, 2, 2, 2])\n",
        "\n",
        "def resnet34(in_channels, n_classes):\n",
        "    return ResNet(in_channels, n_classes, block=ResNetBasicBlock, deepths=[3, 4, 6, 3])\n",
        "\n",
        "def resnet50(in_channels, n_classes):\n",
        "    return ResNet(in_channels, n_classes, block=ResNetBottleNeckBlock, deepths=[3, 4, 6, 3])\n",
        "\n",
        "def resnet101(in_channels, n_classes):\n",
        "    return ResNet(in_channels, n_classes, block=ResNetBottleNeckBlock, deepths=[3, 4, 23, 3])\n",
        "\n",
        "def resnet152(in_channels, n_classes):\n",
        "    return ResNet(in_channels, n_classes, block=ResNetBottleNeckBlock, deepths=[3, 8, 36, 3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1qP7U3n49Xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class AverageMeter(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def train_epoch(model, loader, optimizer, epoch, n_epochs, print_freq=10):\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    error = AverageMeter()\n",
        "\n",
        "    # Model on train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for batch_idx, (input, target) in enumerate(loader):\n",
        "        input = F.upsample(input,size = (224,224))\n",
        "        # Create vaiables\n",
        "        if torch.cuda.is_available():\n",
        "            input = input.cuda()\n",
        "            target = target.cuda()\n",
        "            \n",
        "        # compute output\n",
        "        output = model(input)\n",
        "        loss = torch.nn.functional.cross_entropy(output, target)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        batch_size = target.size(0)\n",
        "        _, pred = output.data.cpu().topk(1, dim=1)\n",
        "        error.update(torch.ne(pred.squeeze(), target.cpu()).float().sum().item() / batch_size, batch_size)\n",
        "        losses.update(loss.item(), batch_size)\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        # print stats\n",
        "        if batch_idx % print_freq == 0:\n",
        "            res = '\\t'.join([\n",
        "                'Epoch: [%d/%d]' % (epoch + 1, n_epochs),\n",
        "                'Iter: [%d/%d]' % (batch_idx + 1, len(loader)),\n",
        "                'Time %.3f (%.3f)' % (batch_time.val, batch_time.avg),\n",
        "                'Loss %.4f (%.4f)' % (losses.val, losses.avg),\n",
        "                'Error %.4f (%.4f)' % (error.val, error.avg),\n",
        "            ])\n",
        "            print(res)\n",
        "\n",
        "    # Return summary statistics\n",
        "    return batch_time.avg, losses.avg, error.avg\n",
        "\n",
        "\n",
        "def test_epoch(model, loader, print_freq=10, is_test=True):\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    error = AverageMeter()\n",
        "\n",
        "    # Model on eval mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (input, target) in enumerate(loader):\n",
        "            input = F.upsample(input,size = (224,224))\n",
        "            # Create vaiables\n",
        "            if torch.cuda.is_available():\n",
        "                input = input.cuda()\n",
        "                target = target.cuda()\n",
        "\n",
        "            # compute output\n",
        "            output = model(input)\n",
        "            loss = torch.nn.functional.cross_entropy(output, target)\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            batch_size = target.size(0)\n",
        "            _, pred = output.data.cpu().topk(1, dim=1)\n",
        "            error.update(torch.ne(pred.squeeze(), target.cpu()).float().sum().item() / batch_size, batch_size)\n",
        "            losses.update(loss.item(), batch_size)\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            # print stats\n",
        "            if batch_idx % print_freq == 0:\n",
        "                res = '\\t'.join([\n",
        "                    'Test' if is_test else 'Valid',\n",
        "                    'Iter: [%d/%d]' % (batch_idx + 1, len(loader)),\n",
        "                    'Time %.3f (%.3f)' % (batch_time.val, batch_time.avg),\n",
        "                    'Loss %.4f (%.4f)' % (losses.val, losses.avg),\n",
        "                    'Error %.4f (%.4f)' % (error.val, error.avg),\n",
        "                ])\n",
        "                print(res)\n",
        "\n",
        "    # Return summary statistics\n",
        "    return batch_time.avg, losses.avg, error.avg\n",
        "\n",
        "\n",
        "def train(model, train_set, valid_set, test_set, save, n_epochs=100,\n",
        "          batch_size=64, lr=0.1, wd=0.0001, momentum=0.9, seed=None):\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True,\n",
        "                                               pin_memory=(torch.cuda.is_available()), num_workers=0)\n",
        "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False,\n",
        "                                              pin_memory=(torch.cuda.is_available()), num_workers=0)\n",
        "    if valid_set is None:\n",
        "        valid_loader = None\n",
        "    else:\n",
        "        valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False,\n",
        "                                                   pin_memory=(torch.cuda.is_available()), num_workers=0)\n",
        "    # Model on cuda\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "\n",
        "    # Wrap model for multi-GPUs, if necessary\n",
        "    model_wrapper = model\n",
        "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
        "        model_wrapper = torch.nn.DataParallel(model).cuda()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.SGD(model_wrapper.parameters(), lr=lr, momentum=momentum, nesterov=True, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[0.5 * n_epochs, 0.75 * n_epochs],\n",
        "                                                     gamma=0.1)\n",
        "\n",
        "    # Start log\n",
        "    with open(os.path.join(save, 'results.csv'), 'w') as f:\n",
        "        f.write('epoch,train_loss,train_error,valid_loss,valid_error,test_error\\n')\n",
        "\n",
        "    # Train model\n",
        "    best_error = 1\n",
        "    for epoch in range(n_epochs):\n",
        "        scheduler.step()\n",
        "        _, train_loss, train_error = train_epoch(\n",
        "            model=model_wrapper,\n",
        "            loader=train_loader,\n",
        "            optimizer=optimizer,\n",
        "            epoch=epoch,\n",
        "            n_epochs=n_epochs,\n",
        "        )\n",
        "        _, valid_loss, valid_error = test_epoch(\n",
        "            model=model_wrapper,\n",
        "            loader=valid_loader if valid_loader else test_loader,\n",
        "            is_test=(not valid_loader)\n",
        "        )\n",
        "\n",
        "        # Determine if model is the best\n",
        "        if valid_loader:\n",
        "            if valid_error < best_error:\n",
        "                best_error = valid_error\n",
        "                print('New best error: %.4f' % best_error)\n",
        "                torch.save(model.state_dict(), os.path.join(save, 'model.dat'))\n",
        "        else:\n",
        "            torch.save(model.state_dict(), os.path.join(save, 'model.dat'))\n",
        "\n",
        "        # Log results\n",
        "        with open(os.path.join(save, 'results.csv'), 'a') as f:\n",
        "            f.write('%03d,%0.6f,%0.6f,%0.5f,%0.5f,\\n' % (\n",
        "                (epoch + 1),\n",
        "                train_loss,\n",
        "                train_error,\n",
        "                valid_loss,\n",
        "                valid_error,\n",
        "            ))\n",
        "\n",
        "    # Final test of model on test set\n",
        "    model.load_state_dict(torch.load(os.path.join(save, 'model.dat')))\n",
        "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
        "        model = torch.nn.DataParallel(model).cuda()\n",
        "    test_results = test_epoch(\n",
        "        model=model,\n",
        "        loader=test_loader,\n",
        "        is_test=True\n",
        "    )\n",
        "    _, _, test_error = test_results\n",
        "    with open(os.path.join(save, 'results.csv'), 'a') as f:\n",
        "        f.write(',,,,,%0.5f\\n' % (test_error))\n",
        "    print('Final test error: %.4f' % test_error)\n",
        "\n",
        "\n",
        "def demo(data, save, depth=100, growth_rate=12, efficient=True, valid_size=5000,\n",
        "         n_epochs=100, batch_size=64, seed=None):\n",
        "\n",
        "    # Data transforms\n",
        "    mean = [0.5071, 0.4867, 0.4408]\n",
        "    stdv = [0.2675, 0.2565, 0.2761]\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=stdv),\n",
        "    ])\n",
        "    test_transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=stdv),\n",
        "    ])\n",
        "\n",
        "    # Datasets\n",
        "    train_set = datasets.CIFAR10(data, train=True, transform=train_transforms, download=True)\n",
        "    test_set = datasets.CIFAR10(data, train=False, transform=test_transforms, download=False)\n",
        "\n",
        "    if valid_size:\n",
        "        valid_set = datasets.CIFAR10(data, train=True, transform=test_transforms)\n",
        "        indices = torch.randperm(len(train_set))\n",
        "        #train_indices = indices[:len(indices) - valid_size]\n",
        "        train_indices = indices[:round(len(indices)*0.5)]\n",
        "        #valid_indices = indices[len(indices) - valid_size:]\n",
        "        valid_indices = indices[round(len(indices)*0.5) + 1:(round(len(indices)*0.5) + 1) + valid_size]\n",
        "        print(valid_indices.size())\n",
        "        train_set = torch.utils.data.Subset(train_set, train_indices)\n",
        "        valid_set = torch.utils.data.Subset(valid_set, valid_indices)\n",
        "    else:\n",
        "        valid_set = None\n",
        "\n",
        "    # Models\n",
        "    model = resnet18(3,10)\n",
        "    print(model)\n",
        "\n",
        "    # Make save directory\n",
        "    if not os.path.exists(save):\n",
        "        os.makedirs(save)\n",
        "    if not os.path.isdir(save):\n",
        "        raise Exception('%s is not a dir' % save)\n",
        "\n",
        "    # Train the model\n",
        "    train(model=model, train_set=train_set, valid_set=valid_set, test_set=test_set, save=save,\n",
        "          n_epochs=n_epochs, batch_size=batch_size, seed=seed)\n",
        "    print('Done!')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "demo('cifar10', './', depth=100, growth_rate=12, efficient=True, valid_size=5000,\n",
        "         n_epochs=100, batch_size=64, seed=None)\n",
        "#if __name__ == '__main__':\n",
        "#    fire.Fire(demo)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}